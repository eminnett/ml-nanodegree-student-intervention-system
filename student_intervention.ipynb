{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2: Supervised Learning\n",
    "### Building a Student Intervention System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classification vs Regression\n",
    "\n",
    "Your goal is to identify students who might need early intervention - which type of supervised machine learning problem is this, classification or regression? Why?\n",
    "\n",
    "This is a classification problem because the output is discreet. Either stdents need early intervention or they don't. Regression problems deal with continuous outputs (ie real numbers) and the output for this problem definitely is not continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploring the Data\n",
    "\n",
    "Let's go ahead and read in the student dataset first.\n",
    "\n",
    "_To execute a code cell, click inside it and press **Shift+Enter**._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student data read successfully!\n"
     ]
    }
   ],
   "source": [
    "# Read student data\n",
    "student_data = pd.read_csv(\"student-data.csv\")\n",
    "print \"Student data read successfully!\"\n",
    "# Note: The last column 'passed' is the target/label, all other are feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, can you find out the following facts about the dataset?\n",
    "- Total number of students\n",
    "- Number of students who passed\n",
    "- Number of students who failed\n",
    "- Graduation rate of the class (%)\n",
    "- Number of features\n",
    "\n",
    "_Use the code block below to compute these values. Instructions/steps are marked using **TODO**s._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of students: 395\n",
      "Number of students who passed: 265\n",
      "Number of students who failed: 130\n",
      "Number of features: 30\n",
      "Graduation rate of the class: 67.09%\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute desired values - replace each '?' with an appropriate expression/function call\n",
    "n_students = len(student_data)\n",
    "n_features = len(student_data.columns[:-1])\n",
    "n_passed = len(student_data[student_data.passed == \"yes\"])\n",
    "n_failed = len(student_data[student_data.passed == \"no\"])\n",
    "grad_rate = 100. * n_passed / n_students\n",
    "print \"Total number of students: {}\".format(n_students)\n",
    "print \"Number of students who passed: {}\".format(n_passed)\n",
    "print \"Number of students who failed: {}\".format(n_failed)\n",
    "print \"Number of features: {}\".format(n_features)\n",
    "print \"Graduation rate of the class: {:.2f}%\".format(grad_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preparing the Data\n",
    "In this section, we will prepare the data for modeling, training and testing.\n",
    "\n",
    "### Identify feature and target columns\n",
    "It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.\n",
    "\n",
    "Let's first separate our data into feature and target columns, and see if any features are non-numeric.<br/>\n",
    "**Note**: For this dataset, the last column (`'passed'`) is the target or label we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature column(s):-\n",
      "['school', 'sex', 'age', 'address', 'famsize', 'Pstatus', 'Medu', 'Fedu', 'Mjob', 'Fjob', 'reason', 'guardian', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n",
      "Target column: passed\n",
      "\n",
      "Feature values:-\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>school</th>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "      <td>GP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sex</th>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>address</th>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famsize</th>\n",
       "      <td>GT3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>LE3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>LE3</td>\n",
       "      <td>LE3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>LE3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>LE3</td>\n",
       "      <td>GT3</td>\n",
       "      <td>GT3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pstatus</th>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>T</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Medu</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fedu</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mjob</th>\n",
       "      <td>at_home</td>\n",
       "      <td>at_home</td>\n",
       "      <td>at_home</td>\n",
       "      <td>health</td>\n",
       "      <td>other</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>teacher</td>\n",
       "      <td>services</td>\n",
       "      <td>health</td>\n",
       "      <td>teacher</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fjob</th>\n",
       "      <td>teacher</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>teacher</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "      <td>health</td>\n",
       "      <td>other</td>\n",
       "      <td>services</td>\n",
       "      <td>other</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reason</th>\n",
       "      <td>course</td>\n",
       "      <td>course</td>\n",
       "      <td>other</td>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>reputation</td>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>home</td>\n",
       "      <td>reputation</td>\n",
       "      <td>reputation</td>\n",
       "      <td>course</td>\n",
       "      <td>course</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guardian</th>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>mother</td>\n",
       "      <td>father</td>\n",
       "      <td>father</td>\n",
       "      <td>mother</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>traveltime</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studytime</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>failures</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>schoolsup</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famsup</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>paid</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>activities</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nursery</th>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>higher</th>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>internet</th>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>romantic</th>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>famrel</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freetime</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>goout</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dalc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Walc</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>health</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>absences</th>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0        1        2         3       4           5       6   \\\n",
       "school           GP       GP       GP        GP      GP          GP      GP   \n",
       "sex               F        F        F         F       F           M       M   \n",
       "age              18       17       15        15      16          16      16   \n",
       "address           U        U        U         U       U           U       U   \n",
       "famsize         GT3      GT3      LE3       GT3     GT3         LE3     LE3   \n",
       "Pstatus           A        T        T         T       T           T       T   \n",
       "Medu              4        1        1         4       3           4       2   \n",
       "Fedu              4        1        1         2       3           3       2   \n",
       "Mjob        at_home  at_home  at_home    health   other    services   other   \n",
       "Fjob        teacher    other    other  services   other       other   other   \n",
       "reason       course   course    other      home    home  reputation    home   \n",
       "guardian     mother   father   mother    mother  father      mother  mother   \n",
       "traveltime        2        1        1         1       1           1       1   \n",
       "studytime         2        2        2         3       2           2       2   \n",
       "failures          0        0        3         0       0           0       0   \n",
       "schoolsup       yes       no      yes        no      no          no      no   \n",
       "famsup           no      yes       no       yes     yes         yes      no   \n",
       "paid             no       no      yes       yes     yes         yes      no   \n",
       "activities       no       no       no       yes      no         yes      no   \n",
       "nursery         yes       no      yes       yes     yes         yes     yes   \n",
       "higher          yes      yes      yes       yes     yes         yes     yes   \n",
       "internet         no      yes      yes       yes      no         yes     yes   \n",
       "romantic         no       no       no       yes      no          no      no   \n",
       "famrel            4        5        4         3       4           5       4   \n",
       "freetime          3        3        3         2       3           4       4   \n",
       "goout             4        3        2         2       2           2       4   \n",
       "Dalc              1        1        2         1       1           1       1   \n",
       "Walc              1        1        3         1       2           2       1   \n",
       "health            3        3        3         5       5           5       3   \n",
       "absences          6        4       10         2       4          10       0   \n",
       "\n",
       "                 7         8       9           10          11        12  \\\n",
       "school           GP        GP      GP          GP          GP        GP   \n",
       "sex               F         M       M           F           F         M   \n",
       "age              17        15      15          15          15        15   \n",
       "address           U         U       U           U           U         U   \n",
       "famsize         GT3       LE3     GT3         GT3         GT3       LE3   \n",
       "Pstatus           A         A       T           T           T         T   \n",
       "Medu              4         3       3           4           2         4   \n",
       "Fedu              4         2       4           4           1         4   \n",
       "Mjob          other  services   other     teacher    services    health   \n",
       "Fjob        teacher     other   other      health       other  services   \n",
       "reason         home      home    home  reputation  reputation    course   \n",
       "guardian     mother    mother  mother      mother      father    father   \n",
       "traveltime        2         1       1           1           3         1   \n",
       "studytime         2         2       2           2           3         1   \n",
       "failures          0         0       0           0           0         0   \n",
       "schoolsup       yes        no      no          no          no        no   \n",
       "famsup          yes       yes     yes         yes         yes       yes   \n",
       "paid             no       yes     yes         yes          no       yes   \n",
       "activities       no        no     yes          no         yes       yes   \n",
       "nursery         yes       yes     yes         yes         yes       yes   \n",
       "higher          yes       yes     yes         yes         yes       yes   \n",
       "internet         no       yes     yes         yes         yes       yes   \n",
       "romantic         no        no      no          no          no        no   \n",
       "famrel            4         4       5           3           5         4   \n",
       "freetime          1         2       5           3           2         3   \n",
       "goout             4         2       1           3           2         3   \n",
       "Dalc              1         1       1           1           1         1   \n",
       "Walc              1         1       1           2           1         3   \n",
       "health            1         1       5           2           4         5   \n",
       "absences          6         0       0           0           4         2   \n",
       "\n",
       "                 13     14  \n",
       "school           GP     GP  \n",
       "sex               M      M  \n",
       "age              15     15  \n",
       "address           U      U  \n",
       "famsize         GT3    GT3  \n",
       "Pstatus           T      A  \n",
       "Medu              4      2  \n",
       "Fedu              3      2  \n",
       "Mjob        teacher  other  \n",
       "Fjob          other  other  \n",
       "reason       course   home  \n",
       "guardian     mother  other  \n",
       "traveltime        2      1  \n",
       "studytime         2      3  \n",
       "failures          0      0  \n",
       "schoolsup        no     no  \n",
       "famsup          yes    yes  \n",
       "paid            yes     no  \n",
       "activities       no     no  \n",
       "nursery         yes    yes  \n",
       "higher          yes    yes  \n",
       "internet        yes    yes  \n",
       "romantic         no    yes  \n",
       "famrel            5      4  \n",
       "freetime          4      5  \n",
       "goout             3      2  \n",
       "Dalc              1      1  \n",
       "Walc              2      1  \n",
       "health            3      3  \n",
       "absences          2      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Extract feature (X) and target (y) columns\n",
    "feature_cols = list(student_data.columns[:-1])  # all columns but last are features\n",
    "target_col = student_data.columns[-1]  # last column is the target/label\n",
    "print \"Feature column(s):-\\n{}\".format(feature_cols)\n",
    "print \"Target column: {}\".format(target_col)\n",
    "\n",
    "X_all = student_data[feature_cols]  # feature values for all students\n",
    "y_all = student_data[target_col]  # corresponding targets/labels\n",
    "print \"\\nFeature values:-\"\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "display(X_all[:15].T)  # print the first 15 rows (as a transpose table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess feature columns\n",
    "\n",
    "As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.\n",
    "\n",
    "Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.\n",
    "\n",
    "These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed feature columns (48):-\n",
      "['school_GP', 'school_MS', 'sex_F', 'sex_M', 'age', 'address_R', 'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T', 'Medu', 'Fedu', 'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services', 'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other', 'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home', 'reason_other', 'reason_reputation', 'guardian_father', 'guardian_mother', 'guardian_other', 'traveltime', 'studytime', 'failures', 'schoolsup', 'famsup', 'paid', 'activities', 'nursery', 'higher', 'internet', 'romantic', 'famrel', 'freetime', 'goout', 'Dalc', 'Walc', 'health', 'absences']\n"
     ]
    }
   ],
   "source": [
    "# Preprocess feature columns\n",
    "def preprocess_features(X):\n",
    "    outX = pd.DataFrame(index=X.index)  # output dataframe, initially empty\n",
    "\n",
    "    # Check each column\n",
    "    for col, col_data in X.iteritems():\n",
    "        # If data type is non-numeric, try to replace all yes/no values with 1/0\n",
    "        if col_data.dtype == object:\n",
    "            col_data = col_data.replace(['yes', 'no'], [1, 0])\n",
    "        # Note: This should change the data type for yes/no columns to int\n",
    "\n",
    "        # If still non-numeric, convert to one or more dummy variables\n",
    "        if col_data.dtype == object:\n",
    "            col_data = pd.get_dummies(col_data, prefix=col)  # e.g. 'school' => 'school_GP', 'school_MS'\n",
    "\n",
    "        outX = outX.join(col_data)  # collect column(s) in output dataframe\n",
    "\n",
    "    return outX\n",
    "\n",
    "X_all = preprocess_features(X_all)\n",
    "print \"Processed feature columns ({}):-\\n{}\".format(len(X_all.columns), list(X_all.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into training and test sets\n",
    "\n",
    "So far, we have converted all _categorical_ features into numeric values. In this next step, we split the data (both features and corresponding labels) into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 300 samples\n",
      "Test set: 95 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# First, decide how many training vs test samples you want\n",
    "num_all = student_data.shape[0]  # same as len(student_data)\n",
    "num_train = 300  # about 75% of the data\n",
    "num_test = num_all - num_train\n",
    "\n",
    "# TODO: Then, select features (X) and corresponding labels (y) for the training and test sets\n",
    "# Note: Shuffle the data or randomly select samples to avoid any bias due to ordering in the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=47)\n",
    "print \"Training set: {} samples\".format(X_train.shape[0])\n",
    "print \"Test set: {} samples\".format(X_test.shape[0])\n",
    "# Note: If you need a validation set, extract it from within training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluating Models\n",
    "Choose 3 supervised learning models that are available in scikit-learn, and appropriate for this problem. For each model:\n",
    "\n",
    "- What are the general applications of this model? What are its strengths and weaknesses?\n",
    "- Given what you know about the data so far, why did you choose this model to apply?\n",
    "- Fit this model to the training data, try to predict labels (for both training and test sets), and measure the F<sub>1</sub> score. Repeat this process with different training set sizes (100, 200, 300), keeping test set constant.\n",
    "\n",
    "Produce a table showing training time, prediction time, F<sub>1</sub> score on training set and F<sub>1</sub> score on test set, for each training set size.\n",
    "\n",
    "Note: You need to produce 3 such tables - one for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVC...\n",
      "Done!\n",
      "Training time (secs): 0.006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00642704963684082"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a model\n",
    "import time\n",
    "\n",
    "def train_classifier(clf, X_train, y_train, print_output=True):\n",
    "    if print_output:\n",
    "        print \"Training {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time.time()\n",
    "    training_time = end - start\n",
    "    if print_output:\n",
    "        print \"Done!\\nTraining time (secs): {:.3f}\".format(training_time)\n",
    "    return training_time\n",
    "\n",
    "# TODO: Choose a model, import it and instantiate an object\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "\n",
    "# Fit model to training data\n",
    "train_classifier(clf, X_train, y_train)  # note: using entire training set here\n",
    "#print clf  # you can inspect the learned model by printing it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.005\n",
      "F1 score for training set: 0.860927152318\n"
     ]
    }
   ],
   "source": [
    "# Predict on training set and compute F1 score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def predict_labels(clf, features, target, print_output=True):\n",
    "    if print_output:\n",
    "        print \"Predicting labels using {}...\".format(clf.__class__.__name__)\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    prediction_time = end - start\n",
    "    if print_output:\n",
    "        print \"Done!\\nPrediction time (secs): {:.3f}\".format(prediction_time)\n",
    "    return (prediction_time, f1_score(target.values, y_pred, pos_label='yes'))\n",
    "\n",
    "train_prediction_time, train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "print \"F1 score for training set: {}\".format(train_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using SVC...\n",
      "Done!\n",
      "Prediction time (secs): 0.002\n",
      "F1 score for test set: 0.828025477707\n"
     ]
    }
   ],
   "source": [
    "# Predict on test data\n",
    "test_prediction_time, test_f1_score = predict_labels(clf, X_test, y_test)\n",
    "print \"F1 score for test set: {}\".format(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train and predict using different training set sizes\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test, print_output=True):\n",
    "    if print_output:\n",
    "        print \"------------------------------------------\"\n",
    "        print \"Training set size: {}\".format(len(X_train))\n",
    "    training_time = train_classifier(clf, X_train, y_train, print_output)\n",
    "    train_prediction_time, train_f1_score = predict_labels(clf, X_train, y_train, print_output)\n",
    "    test_prediction_time, test_f1_score = predict_labels(clf, X_test, y_test, print_output)\n",
    "    if print_output:\n",
    "        print \"F1 score for training set: {}\".format(train_f1_score)\n",
    "        print \"F1 score for test set: {}\".format(test_f1_score)\n",
    "    return (training_time, train_prediction_time, train_f1_score, test_prediction_time, test_f1_score)\n",
    "\n",
    "# TODO: Run the helper function above for desired subsets of training data\n",
    "# Note: Keep the test set constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Train and predict using two other models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Model Investigation and Comparative Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we know the following about the student intervention classification. We know that the target for the classification model is a single binary class (either a student has passed or they have not). We also know that all of the input features are categorical. Even a student's age can be treated as categorical given that the values are integers and the range of ages for students is reasonably small. Given the large set of categorical features, it is unlikely the data set contains a linear decision boundary between the two target classes. For this reason we will not include any linear classifiers in this exploration.\n",
    "\n",
    "We do not have a prior model for what makes a student pass or not. For this reason non-parametric models are valid candidates for the Student Intervention Classifier. In general non-parametric models make fewer assumptions about the population from which a sample is taken [1].\n",
    "\n",
    "We will also consider the potential of Ensemble Learning Models and that one of them may be the most appropriate classifier. The ensemble method uses multiple algorithms in an attempt to improve model performance. Even though multiple algorithms may improve performance, that performance comes at a cost (primarily training and prediction time) [2].\n",
    "\n",
    "In an attempt to be rigorous in this investigation, it feels prudent to investigate more than 3 classification algorithms especially as the cost of adding additional algorithms to the process is relatively low. For this reason and given the details for this classification problem discussed above, we will investigate the following algorithms: K Nearest Neighbors, SVM, Random Forest, AdaBoost, and Naive Bayes. These 5 algorithms include include two non-parametric models, two ensemble learning models, and Naive Bayes is a relatively simple probabilistic model. We will use Naive Bayes as a benchmark as it is well understood. We will now compare the general applications, strengths, weaknesses, justifications and general performance of these algorithms using their default configurations (as implemented in the SKLearn library). The performance will be calculated with the first 100, first 200 and all 300 training records. Our hope is that from this investigation there will be one algorithm from the 5 that will present itself as the most appropriate choice as the Student Intervention Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Nearest Neighbors</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Support Vector Machine</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Random Forest</th>\n",
       "      <th colspan=\"3\" halign=\"left\">AdaBoost</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Naive Bayes</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General.Applications</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Primarily used for classification when the similarity between neighboring data is relevant and when a meaningful measurement of distance can be defined [3].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Typically used for problems with a target variable of  2 classes with a well defined decision boundary [4].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">A robust predictive model especially suitable for problems where overfitting is a concern [5].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">AdaBoost is 'often referred to as the best out-of-the-box classifier' [7] and can be applied to a wide variety of classification problems.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">A classic use of Naive Bayse is for text classification and categorisation [9].</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Strengths</th>\n",
       "      <th colspan=\"3\" halign=\"left\">East to understand. Fast to train. [3]</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Maximisation of generalisation ability. No local minima. Robustness to outliers. [4]</th>\n",
       "      <th colspan=\"3\" halign=\"left\">The use fo multiple trees results in increased predictive power and reduces the prediction variance [6].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Boosting can be seen as from of L1 regularization which helps prevent overfitting and eliminating 'irrelevant' features[6]. Maximises the margin on the training set [8].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Fast to train. Easy to mix and match features of different types [10].</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Weaknesses</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Need to determine K. Need to decide how distance is measured and defined. Sensitive to local structure in the data. The data is required for predictions. [3]</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Slow to train. Have to decide on kernel and parameter selection. [4]</th>\n",
       "      <th colspan=\"3\" halign=\"left\">The use fo multiple trees results in the loss interpretability of a single decision tree [6]. Need to decide how many trees and their max depth.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Sensitive to noisy data and outliers [7].</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Can suffer from overfitting [11]. The run-time cost may be too high for some applications [11].</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Justification</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Non-parametric model. See explanation above.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Non-parametric model. See explanation above.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Ensemble method. See explanation above.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Ensemble method. See explanation above.</th>\n",
       "      <th colspan=\"3\" halign=\"left\">Reasonably simple probabalistic model. Will act as a benchmark for the other models.</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train.Set.Size</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "      <th>100</th>\n",
       "      <th>200</th>\n",
       "      <th>300</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Train.Time</th>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.005467</td>\n",
       "      <td>0.015192</td>\n",
       "      <td>0.015780</td>\n",
       "      <td>0.016906</td>\n",
       "      <td>0.100497</td>\n",
       "      <td>0.078302</td>\n",
       "      <td>0.082699</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.000725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train.Pred.Time</th>\n",
       "      <td>0.001669</td>\n",
       "      <td>0.003145</td>\n",
       "      <td>0.005144</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.002013</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.003950</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.005063</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>0.000413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Train.Pred.F1</th>\n",
       "      <td>0.867925</td>\n",
       "      <td>0.852349</td>\n",
       "      <td>0.846682</td>\n",
       "      <td>0.858896</td>\n",
       "      <td>0.856209</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.992806</td>\n",
       "      <td>0.988506</td>\n",
       "      <td>0.984848</td>\n",
       "      <td>0.992908</td>\n",
       "      <td>0.890511</td>\n",
       "      <td>0.859189</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.827839</td>\n",
       "      <td>0.783920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test.Pred.Time</th>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Test.Pred.F1</th>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.834356</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.828025</td>\n",
       "      <td>0.748201</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>0.715328</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.813793</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.731343</td>\n",
       "      <td>0.738462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Name                                                                                                                                                             Nearest Neighbors  \\\n",
       "General.Applications  Primarily used for classification when the similarity between neighboring data is relevant and when a meaningful measurement of distance can be defined [3].   \n",
       "Strengths                                                                                                                                   East to understand. Fast to train. [3]   \n",
       "Weaknesses           Need to determine K. Need to decide how distance is measured and defined. Sensitive to local structure in the data. The data is required for predictions. [3]   \n",
       "Justification                                                                                                                         Non-parametric model. See explanation above.   \n",
       "Train.Set.Size                                                                                                                                                                 100   \n",
       "Train.Time                                                     0.000725                                                                                                              \n",
       "Train.Pred.Time                                                0.001669                                                                                                              \n",
       "Train.Pred.F1                                                  0.867925                                                                                                              \n",
       "Test.Pred.Time                                                 0.001261                                                                                                              \n",
       "Test.Pred.F1                                                   0.818182                                                                                                              \n",
       "\n",
       "Name                                      \\\n",
       "General.Applications                       \n",
       "Strengths                                  \n",
       "Weaknesses                                 \n",
       "Justification                              \n",
       "Train.Set.Size             200       300   \n",
       "Train.Time            0.000511  0.000690   \n",
       "Train.Pred.Time       0.003145  0.005144   \n",
       "Train.Pred.F1         0.852349  0.846682   \n",
       "Test.Pred.Time        0.001690  0.002573   \n",
       "Test.Pred.F1          0.777778  0.837838   \n",
       "\n",
       "Name                                                                                                      Support Vector Machine  \\\n",
       "General.Applications Typically used for problems with a target variable of  2 classes with a well defined decision boundary [4].   \n",
       "Strengths                                   Maximisation of generalisation ability. No local minima. Robustness to outliers. [4]   \n",
       "Weaknesses                                                  Slow to train. Have to decide on kernel and parameter selection. [4]   \n",
       "Justification                                                                       Non-parametric model. See explanation above.   \n",
       "Train.Set.Size                                                                                                               100   \n",
       "Train.Time                                                     0.001074                                                            \n",
       "Train.Pred.Time                                                0.000646                                                            \n",
       "Train.Pred.F1                                                  0.858896                                                            \n",
       "Test.Pred.Time                                                 0.000612                                                            \n",
       "Test.Pred.F1                                                   0.834356                                                            \n",
       "\n",
       "Name                                      \\\n",
       "General.Applications                       \n",
       "Strengths                                  \n",
       "Weaknesses                                 \n",
       "Justification                              \n",
       "Train.Set.Size             200       300   \n",
       "Train.Time            0.002755  0.005467   \n",
       "Train.Pred.Time       0.002013  0.004124   \n",
       "Train.Pred.F1         0.856209  0.860927   \n",
       "Test.Pred.Time        0.001039  0.001365   \n",
       "Test.Pred.F1          0.822785  0.828025   \n",
       "\n",
       "Name                                                                                                                                                    Random Forest  \\\n",
       "General.Applications                                                   A robust predictive model especially suitable for problems where overfitting is a concern [5].   \n",
       "Strengths                                                    The use fo multiple trees results in increased predictive power and reduces the prediction variance [6].   \n",
       "Weaknesses           The use fo multiple trees results in the loss interpretability of a single decision tree [6]. Need to decide how many trees and their max depth.   \n",
       "Justification                                                                                                                 Ensemble method. See explanation above.   \n",
       "Train.Set.Size                                                                                                                                                    100   \n",
       "Train.Time                                                     0.015192                                                                                                 \n",
       "Train.Pred.Time                                                0.000858                                                                                                 \n",
       "Train.Pred.F1                                                  0.992806                                                                                                 \n",
       "Test.Pred.Time                                                 0.000714                                                                                                 \n",
       "Test.Pred.F1                                                   0.748201                                                                                                 \n",
       "\n",
       "Name                                      \\\n",
       "General.Applications                       \n",
       "Strengths                                  \n",
       "Weaknesses                                 \n",
       "Justification                              \n",
       "Train.Set.Size             200       300   \n",
       "Train.Time            0.015780  0.016906   \n",
       "Train.Pred.Time       0.001125  0.001187   \n",
       "Train.Pred.F1         0.988506  0.984848   \n",
       "Test.Pred.Time        0.000733  0.000733   \n",
       "Test.Pred.F1          0.715328  0.771429   \n",
       "\n",
       "Name                                                                                                                                                                                  AdaBoost  \\\n",
       "General.Applications                                AdaBoost is 'often referred to as the best out-of-the-box classifier' [7] and can be applied to a wide variety of classification problems.   \n",
       "Strengths            Boosting can be seen as from of L1 regularization which helps prevent overfitting and eliminating 'irrelevant' features[6]. Maximises the margin on the training set [8].   \n",
       "Weaknesses                                                                                                                                           Sensitive to noisy data and outliers [7].   \n",
       "Justification                                                                                                                                          Ensemble method. See explanation above.   \n",
       "Train.Set.Size                                                                                                                                                                             100   \n",
       "Train.Time                                                     0.100497                                                                                                                          \n",
       "Train.Pred.Time                                                0.003950                                                                                                                          \n",
       "Train.Pred.F1                                                  0.992908                                                                                                                          \n",
       "Test.Pred.Time                                                 0.003904                                                                                                                          \n",
       "Test.Pred.F1                                                   0.715328                                                                                                                          \n",
       "\n",
       "Name                                      \\\n",
       "General.Applications                       \n",
       "Strengths                                  \n",
       "Weaknesses                                 \n",
       "Justification                              \n",
       "Train.Set.Size             200       300   \n",
       "Train.Time            0.078302  0.082699   \n",
       "Train.Pred.Time       0.004481  0.005063   \n",
       "Train.Pred.F1         0.890511  0.859189   \n",
       "Test.Pred.Time        0.004017  0.003908   \n",
       "Test.Pred.F1          0.742857  0.813793   \n",
       "\n",
       "Name                                                                                                     Naive Bayes  \\\n",
       "General.Applications                 A classic use of Naive Bayse is for text classification and categorisation [9].   \n",
       "Strengths                                     Fast to train. Easy to mix and match features of different types [10].   \n",
       "Weaknesses           Can suffer from overfitting [11]. The run-time cost may be too high for some applications [11].   \n",
       "Justification                   Reasonably simple probabalistic model. Will act as a benchmark for the other models.   \n",
       "Train.Set.Size                                                                                                   100   \n",
       "Train.Time                                                     0.000547                                                \n",
       "Train.Pred.Time                                                0.000237                                                \n",
       "Train.Pred.F1                                                  0.622642                                                \n",
       "Test.Pred.Time                                                 0.000237                                                \n",
       "Test.Pred.F1                                                   0.415842                                                \n",
       "\n",
       "Name                                      \n",
       "General.Applications                      \n",
       "Strengths                                 \n",
       "Weaknesses                                \n",
       "Justification                             \n",
       "Train.Set.Size             200       300  \n",
       "Train.Time            0.000610  0.000725  \n",
       "Train.Pred.Time       0.000336  0.000413  \n",
       "Train.Pred.F1         0.827839  0.783920  \n",
       "Test.Pred.Time        0.000247  0.000241  \n",
       "Test.Pred.F1          0.731343  0.738462  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# For the sake of consistency and as there is little reason not to, this investigation will consider the \n",
    "# classifiers using their default settings.\n",
    "\n",
    "classifiers = [\n",
    "        {\"name\": \"Nearest Neighbors\",\n",
    "            \"clf\": KNeighborsClassifier(), \n",
    "            \"general_applications\": \"Primarily used for classification when the similarity between neighboring \" \\\n",
    "                 \"data is relevant and when a meaningful measurement of distance can be defined [3].\",\n",
    "            \"strengths\": \"East to understand. Fast to train. [3]\",\n",
    "            \"weaknesses\": \"Need to determine K. Need to decide how distance is measured and defined. \"           \\\n",
    "                \"Sensitive to local structure in the data. The data is required for predictions. [3]\",\n",
    "            \"justification\": \"Non-parametric model. See explanation above.\"},\n",
    "        {\"name\": \"Support Vector Machine\",\n",
    "            \"clf\": SVC(), \n",
    "            \"general_applications\": \"Typically used for problems with a target variable of  2 classes with a \"   \\\n",
    "                 \"well defined decision boundary [4].\",\n",
    "            \"strengths\": \"Maximisation of generalisation ability. No local minima. Robustness to outliers. [4]\",\n",
    "            \"weaknesses\": \"Slow to train. Have to decide on kernel and parameter selection. [4]\",\n",
    "            \"justification\": \"Non-parametric model. See explanation above.\"},\n",
    "        {\"name\": \"Random Forest\",\n",
    "            \"clf\": RandomForestClassifier(), \n",
    "            \"general_applications\": \"A robust predictive model especially suitable for problems where \"          \\\n",
    "                 \"overfitting is a concern [5].\",\n",
    "            \"strengths\": \"The use fo multiple trees results in increased predictive power and reduces the \"      \\\n",
    "                 \"prediction variance [6].\",\n",
    "            \"weaknesses\": \"The use fo multiple trees results in the loss interpretability of a single \"          \\\n",
    "                 \"decision tree [6]. Need to decide how many trees and their max depth.\",\n",
    "            \"justification\": \"Ensemble method. See explanation above.\"},\n",
    "        {\"name\": \"AdaBoost\",\n",
    "            \"clf\": AdaBoostClassifier(), \n",
    "            \"general_applications\": \"AdaBoost is 'often referred to as the best out-of-the-box classifier' [7] \" \\\n",
    "                 \"and can be applied to a wide variety of classification problems.\",\n",
    "            \"strengths\": \"Boosting can be seen as from of L1 regularization which helps prevent overfitting \"    \\\n",
    "                 \"and eliminating 'irrelevant' features[6]. Maximises the margin on the training set [8].\",\n",
    "            \"weaknesses\": \"Sensitive to noisy data and outliers [7].\",\n",
    "            \"justification\": \"Ensemble method. See explanation above.\"},\n",
    "        {\"name\": \"Naive Bayes\",\n",
    "            \"clf\": GaussianNB(), \n",
    "            \"general_applications\": \"A classic use of Naive Bayse is for text classification and \"               \\\n",
    "                 \"categorisation [9].\",\n",
    "            \"strengths\": \"Fast to train. Easy to mix and match features of different types [10].\",\n",
    "            \"weaknesses\": \"Can suffer from overfitting [11]. The run-time cost may be too high for some \"        \\\n",
    "                 \"applications [11].\",\n",
    "            \"justification\": \"Reasonably simple probabalistic model. Will act as a benchmark for the other models.\"}\n",
    "    ]\n",
    "\n",
    "df_columns = [\"Name\", \"General.Applications\", \"Strengths\", \"Weaknesses\", \"Justification\", \"Train.Set.Size\", \n",
    "              \"Train.Time\", \"Train.Pred.Time\", \"Train.Pred.F1\", \"Test.Pred.Time\", \"Test.Pred.F1\"]\n",
    "df_indices = df_columns[:6]\n",
    "classifier_investigation_df = pd.DataFrame(columns=df_columns)\n",
    "\n",
    "for classifier in classifiers:\n",
    "    row = {col:0 for col in df_columns}\n",
    "    row[\"Name\"] = classifier[\"name\"]\n",
    "    row[\"General.Applications\"] = classifier[\"general_applications\"]\n",
    "    row[\"Strengths\"] = classifier[\"strengths\"]\n",
    "    row[\"Weaknesses\"] = classifier[\"weaknesses\"]\n",
    "    row[\"Justification\"] = classifier[\"justification\"]\n",
    "    clf = classifier[\"clf\"]\n",
    "    for n in [100, 200, 300]:\n",
    "        row[\"Train.Set.Size\"] = n\n",
    "        training_times_and_scores = train_predict(clf, X_train[:n], y_train[:n], X_test, y_test, False)\n",
    "        row[\"Train.Time\"] = training_times_and_scores[0]\n",
    "        row[\"Train.Pred.Time\"] = training_times_and_scores[1]\n",
    "        row[\"Train.Pred.F1\"] = training_times_and_scores[2]\n",
    "        row[\"Test.Pred.Time\"] = training_times_and_scores[3]\n",
    "        row[\"Test.Pred.F1\"] = training_times_and_scores[4]\n",
    "        \n",
    "        df_length = len(classifier_investigation_df)\n",
    "        classifier_investigation_df.loc[df_length] = row\n",
    "        \n",
    "indexed_df = classifier_investigation_df.set_index(df_indices)\n",
    "indexed_df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrema for each of the 5 performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train.Time\n",
      "Minimum: 0.000510931015015 (Nearest Neighbors | 200.0)\n",
      "Maximum: 0.10049700737 (AdaBoost | 100.0)\n",
      "\n",
      "\n",
      "Train.Pred.Time\n",
      "Minimum: 0.000236988067627 (Naive Bayes | 100.0)\n",
      "Maximum: 0.00514388084412 (Nearest Neighbors | 300.0)\n",
      "\n",
      "\n",
      "Train.Pred.F1\n",
      "Minimum: 0.622641509434 (Naive Bayes | 100.0)\n",
      "Maximum: 0.992907801418 (AdaBoost | 100.0)\n",
      "\n",
      "\n",
      "Test.Pred.Time\n",
      "Minimum: 0.000236988067627 (Naive Bayes | 100.0)\n",
      "Maximum: 0.0040168762207 (AdaBoost | 200.0)\n",
      "\n",
      "\n",
      "Test.Pred.F1\n",
      "Minimum: 0.415841584158 (Naive Bayes | 100.0)\n",
      "Maximum: 0.837837837838 (Nearest Neighbors | 300.0)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_of_rows = classifier_investigation_df.T.to_dict().values()\n",
    "for field in [\"Train.Time\", \"Train.Pred.Time\", \"Train.Pred.F1\", \"Test.Pred.Time\", \"Test.Pred.F1\"]:\n",
    "    print field\n",
    "    min_val = classifier_investigation_df[field].min()\n",
    "    min_row = [r for r in list_of_rows if r[field] == min_val][0]\n",
    "    print \"Minimum: \" + str(min_val) + \" (\" + min_row[\"Name\"] + \" | \" + str(min_row[\"Train.Set.Size\"]) + \")\"\n",
    "    max_val = classifier_investigation_df[field].max()\n",
    "    max_row = [r for r in list_of_rows if r[field] == max_val][0]\n",
    "    print \"Maximum: \" + str(max_val) + \" (\" + max_row[\"Name\"] + \" | \" + str(max_row[\"Train.Set.Size\"]) + \")\"\n",
    "    print \"\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing the Best Model\n",
    "\n",
    "- Based on the experiments you performed earlier, in 1-2 paragraphs explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?\n",
    "- In 1-2 paragraphs explain to the board of supervisors in layman's terms how the final model chosen is supposed to work (for example if you chose a Decision Tree or Support Vector Machine, how does it make a prediction).\n",
    "- Fine-tune the model. Use Gridsearch with at least one important parameter tuned and with at least 3 settings. Use the entire training set for this.\n",
    "- What is the model's final F<sub>1</sub> score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Memo to the Board of Supervisors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We Should Apply K-Nearest Neighbors as the Student Intervention Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results gathered comparing K-Nearest Neighbors, Support Vector Machine, Random Forests, AdaBoost, and Naive Bayes classifiers we have found that the K-Nearest Neighbors offers the greatest potential as the Student Intervention Classifier. Of those 5 algorithms, KNN is very close to the lowest training time and has the highest test prediction F1 score. The F1 score takes a value from 0 to 1 where 1 is better than 0. Models with high precion and recall, or in other words high proportional true positives and true negatives, have higher F1 scores [13]. Even though KNN has a higher prediction time than some of the other algorithms, it still feels appropriate to select it. The low training time will allow hyperparameter optimisation via grid search to be executed faster allowing us to test a greater number of parameters without having to compromise on the cost to do so. Fundamentally, the high F1 score is one of the greatest benefits of choosing KNN.\n",
    "\n",
    "As well as the benefits offered by KNN with respect to performance and cost, KNN is also a non-parametric model. Non-parametric models make fewer assumptions about the population from which the training sample is taken (when compared to parametric models) [1]. Given that we do not have a prior model for what makes a student pass of not, the use of a non-parametric model is ideal. It follows the basic intuition about the Student Intervention problem that students are likely to share the classification with other students with whom they are most similar. In addition to all of the eveidence in favour of KNN, KNN reflects this same intuition when determining how to predict the classification of a new datum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How K-Nearest Neighbors Helps us Classify Students"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the KNN algorithm receives a request to predict the classification of a new datum, it searches the set of available data with known classifications. From this search of the labeled data, a subset of K data are selected that are deemed to be the most similar to the unclassified datum. A single classification is then derived from the data in this subset and that classification is returned as the predicted class for the unclassified datum. At face value, this process is very straight forward. What makes the process a little tricky is deciding how we determine the following parameters: how the search is performed, what is the value of K, how is similarity determined, and how is a single classification derived from the K most similar data. For each of these unknowns there is a finite set of options. The easiest way to find the optimal configuration is what is erferred to as hyperparameter optimisation. In effect, every combination of the available parameter options are tested and the combination that yields the best performing algorithm is chosen [14]. Once we have an optimised configuration for KNN, we can calculate the F1 score against the test data and see how well we can expect the algorithm to generalise the Student Inetrvention Classification for new students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Hyperparameter Optimisation on the KNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Fine-tune your model and report the best F1 score\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn import grid_search\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "clf = KNeighborsClassifier()\n",
    "# The 4 parameters that are discussed above: K, the search algorithm, how similarity is calculated, and how a \n",
    "# single classification is derived.\n",
    "search_parameters = {\n",
    "    'n_neighbors': range(1, 30), \n",
    "    'algorithm': ['auto', 'ball_tree', 'brute'],\n",
    "    # A selection of distance metrics that are conceptually consistent for categorical data [15][16][17] along with \n",
    "    # euclidean and manhattan. We can use distances like euclidean and manhattan because the categorical  \n",
    "    # variables have been converted to dummy variables, not including them results in a higher test F1 score.    \n",
    "    'metric': ['euclidean', 'manhattan', 'hamming', 'jaccard', 'dice', 'kulsinski'],\n",
    "    'weights': ['uniform' , 'distance']\n",
    "}\n",
    "\n",
    "scorer = make_scorer(f1_score, pos_label='yes')\n",
    "ssscv = StratifiedShuffleSplit( y_train, n_iter=10, test_size=0.1)\n",
    "knn_grid_search = grid_search.GridSearchCV(clf, search_parameters, scoring=scorer, cv=ssscv)\n",
    "knn_grid_search = knn_grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised KNN Model Parameters: {'n_neighbors': 19, 'metric': 'manhattan', 'weights': 'uniform', 'algorithm': 'brute'}\n",
      "Optimised KNN Model Training Set F1 Score: 0.83106817195\n"
     ]
    }
   ],
   "source": [
    "print \"Optimised KNN Model Parameters: \" + str(knn_grid_search.best_params_)\n",
    "print \"Optimised KNN Model Training Set F1 Score: \" + str(knn_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimised KNN Test F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimised KNN Model Test Set F1 Score: 0.822784810127\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_grid_search.best_estimator_.predict(X_test)\n",
    "test_f1_score = f1_score(y_test, y_pred, pos_label='yes')\n",
    "print \"Optimised KNN Model Test Set F1 Score: \" + str(test_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A note from the reviewer\n",
    "# Pro Tip (Advanced): You could actually go well beyond grid search and implement ‘pipelines’ where the whole machine \n",
    "# learning process becomes 'grid-searchable' and you can parameterize and search the whole process though cross \n",
    "# validation.\n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html\n",
    "\n",
    "# And yes you can try out several algorithms automatically as well too! Watch out though this is pretty \n",
    "# advanced stuff, here is a great, informative, top notch tutorial from Zac Sewart!\n",
    "# http://zacstewart.com/2014/08/05/pipelines-of-featureunions-of-pipelines.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### References:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] Wikipedia contributors, \"Nonparametric statistics,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Nonparametric_statistics&oldid=704406561 (accessed March 6, 2016).\n",
    "\n",
    "[2] Wikipedia contributors, \"Ensemble learning,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Ensemble_learning&oldid=707462154 (accessed March 6, 2016).\n",
    "\n",
    "[3] Wikipedia contributors, \"K-nearest neighbors algorithm,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=K-nearest_neighbors_algorithm&oldid=708512163 (accessed March 6, 2016).\n",
    "\n",
    "[4] Abe, Shigeo. \"1 Introduction.\" In *Support Vector Machines for Pattern Classification*, 3. London: Springer, 2005.\n",
    "\n",
    "[5] Abe, Shigeo. \"2.5 Two Class Support Vector Machines - Advantages and Disadvantages.\" In *Support Vector Machines for Pattern Classification*, 39. London: Springer, 2005.\n",
    "\n",
    "[6] Breiman, Leo. \"Random Forests.\" *Machine Learning* 45, no. 1 (2001): 5-32. doi:10.1023/a:1010933404324.\n",
    "\n",
    "[7] Murphy, Kevin P. \"16.2.5 Classification and Regression Trees (CART) - Random Forests.\" In *Machine Learning A Probabilistic Perspective*, 552. Cambridge, Mass: MIT Press, 2012.\n",
    "\n",
    "[8] Wikipedia contributors, \"AdaBoost,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=AdaBoost&oldid=707326699 (accessed March 6, 2016).\n",
    "\n",
    "[9] Murphy, Kevin P. \"16.4.8 Adaptive basis function models - Why does boosting work so well?\" In *Machine Learning A Probabilistic Perspective*, 564. Cambridge, Mass: MIT Press, 2012.\n",
    "\n",
    "[10] Wikipedia contributors, \"Naive Bayes classifier,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Naive_Bayes_classifier&oldid=707015212 (accessed March 6, 2016).\n",
    "\n",
    "[11] Murphy, Kevin P. \"3.5 Naive Bayes classifiers.\" In *Machine Learning A Probabilistic Perspective*, 84. Cambridge, Mass: MIT Press, 2012.\n",
    "\n",
    "[12] Murphy, Kevin P. \"3.5.4 Naive Bayes classifiers - Feature selection using mutual information.\" In *Machine Learning A Probabilistic Perspective*, 89. Cambridge, Mass: MIT Press, 2012.\n",
    "\n",
    "[13] Wikipedia contributors, \"F1 score,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=F1_score&oldid=703225135 (accessed March 6, 2016).\n",
    "\n",
    "[14] Wikipedia contributors, \"Hyperparameter optimization,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Hyperparameter_optimization&oldid=705078252 (accessed March 6, 2016).\n",
    "\n",
    "[15] Rajaraman, Anand, and Jeffrey D. Ullman. \"3.5.3 Jaccard Distance.\" In *Mining of Massive Datasets*, 94. New York, N.Y.: Cambridge University Press, 2012.\n",
    "\n",
    "[16] Rajaraman, Anand, and Jeffrey D. Ullman. \"3.5.6 Hamming Distance.\" In *Mining of Massive Datasets*, 96. New York, N.Y.: Cambridge University Press, 2012.\n",
    "\n",
    "[17] Wikipedia contributors, \"Qualitative variation,\" Wikipedia, The Free Encyclopedia, https://en.wikipedia.org/w/index.php?title=Qualitative_variation&oldid=705908713 (accessed March 6, 2016)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
